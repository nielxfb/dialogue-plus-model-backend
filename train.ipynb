{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>468035</th>\n",
       "      <td>a very old woman called you myself... jdjsskak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66372</th>\n",
       "      <td>omen have mouths and anuses just as appealing ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172143</th>\n",
       "      <td>gamergate why why go to all this effort when y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449616</th>\n",
       "      <td>beto mira</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539261</th>\n",
       "      <td>as a mexican and now naturalized american i fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492017</th>\n",
       "      <td>let us talk here about opposite of concepts li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171613</th>\n",
       "      <td>that s right wikipedia is a collaborative sic ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331740</th>\n",
       "      <td>suck my cock motherfuckers why do not you fuck...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341795</th>\n",
       "      <td>i disagree there is a clear geographic compone...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226250</th>\n",
       "      <td>okay then i guess i shall have to redeem myself</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4999 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Content  Label\n",
       "468035  a very old woman called you myself... jdjsskak...      1\n",
       "66372   omen have mouths and anuses just as appealing ...      1\n",
       "172143  gamergate why why go to all this effort when y...      0\n",
       "449616                                          beto mira      1\n",
       "539261  as a mexican and now naturalized american i fo...      1\n",
       "...                                                   ...    ...\n",
       "492017  let us talk here about opposite of concepts li...      1\n",
       "171613  that s right wikipedia is a collaborative sic ...      0\n",
       "331740  suck my cock motherfuckers why do not you fuck...      1\n",
       "341795  i disagree there is a clear geographic compone...      0\n",
       "226250    okay then i guess i shall have to redeem myself      0\n",
       "\n",
       "[4999 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('HateSpeechDatasetBalanced.csv').sample(n=5000)\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def preprocess_words(words):\n",
    "    words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    words = [word for word in words if word.lower() not in string.punctuation]\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    word_tag = pos_tag(words)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in word_tag]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.00%\n"
     ]
    }
   ],
   "source": [
    "tweets = df['Content']\n",
    "labels = df['Label']\n",
    "\n",
    "word_list = []\n",
    "\n",
    "for sentence in tweets:\n",
    "    check_words = word_tokenize(sentence)\n",
    "    for word in check_words:\n",
    "        word_list.append(word)\n",
    "\n",
    "word_list = preprocess_words(word_list)\n",
    "\n",
    "labeled_data = zip(tweets, labels)\n",
    "\n",
    "feature_sets = []\n",
    "\n",
    "for tweet, label in labeled_data:\n",
    "    feature = {}\n",
    "\n",
    "    check_words = word_tokenize(tweet)\n",
    "    check_words = preprocess_words(check_words)\n",
    "\n",
    "    for word in word_list:\n",
    "        feature[word] = word in check_words\n",
    "\n",
    "    feature_sets.append((feature, label))\n",
    "\n",
    "random.shuffle(feature_sets)\n",
    "\n",
    "train_count = int(len(feature_sets) * 0.8)\n",
    "train_set = feature_sets[:train_count]\n",
    "test_set = feature_sets[train_count:]\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print(f\"Accuracy: {accuracy(classifier, test_set) * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('model.pickle', 'wb')\n",
    "pickle.dump(classifier, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
